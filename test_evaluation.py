"""
Script de prueba para el m√≥dulo de evaluaci√≥n.

Demuestra c√≥mo usar las funciones de evaluaci√≥n para analizar
el rendimiento de los modelos entrenados.

Author: Machine Learning Workshop
Date: 2025-10-29
"""

import os
import sys

# Agregar src al path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

import joblib
from evaluation import (
    evaluate_model,
    evaluate_all_models,
    print_detailed_metrics,
    plot_confusion_matrix,
    plot_roc_curves,
    compare_models_table
)


def main():
    """Pipeline completo de evaluaci√≥n de modelos."""
    
    print("\n" + "="*80)
    print("üìä EVALUACI√ìN COMPLETA DE MODELOS - CLASIFICACI√ìN DE RESE√ëAS")
    print("="*80 + "\n")
    
    model_path = "models/review_model.joblib"
    
    # ========== VERIFICAR MODELO EXISTE ==========
    if not os.path.exists(model_path):
        print(f"‚ùå ERROR: No se encontr√≥ el modelo en '{model_path}'")
        print(f"   Ejecuta primero: python test_train_supervised.py")
        return
    
    # ========== CARGAR MODELO Y TEST SET ==========
    print("üìÇ Cargando modelo entrenado...")
    model_data = joblib.load(model_path)
    
    trained_models = model_data['models']
    X_test = model_data['X_test']
    y_test = model_data['y_test']
    metadata = model_data.get('metadata', {})
    
    print(f"‚úì Modelo cargado exitosamente")
    print(f"   ‚Ä¢ Modelos entrenados: {len(trained_models)}")
    print(f"   ‚Ä¢ Test samples: {X_test.shape[0]:,}")
    print(f"   ‚Ä¢ Features: {X_test.shape[1]:,}")
    
    if metadata:
        print(f"   ‚Ä¢ Entrenado: {metadata.get('trained_at', 'N/A')}")
        print(f"   ‚Ä¢ Train samples: {metadata.get('train_samples', 'N/A'):,}")
    
    # ========== EVALUACI√ìN 1: TODOS LOS MODELOS ==========
    print("\n" + "üîµ"*40)
    print("EVALUACI√ìN 1: Comparaci√≥n de todos los modelos")
    print("üîµ"*40 + "\n")
    
    results = evaluate_all_models(trained_models, X_test, y_test, verbose=True)
    
    # ========== EVALUACI√ìN 2: M√âTRICAS DETALLADAS POR MODELO ==========
    print("\n" + "üü¢"*40)
    print("EVALUACI√ìN 2: M√©tricas detalladas de cada modelo")
    print("üü¢"*40)
    
    for model_name, metrics in results.items():
        print_detailed_metrics(metrics)
        input("\nPresiona Enter para continuar al siguiente modelo...")
    
    # ========== EVALUACI√ìN 3: TABLA COMPARATIVA ==========
    print("\n" + "üü°"*40)
    print("EVALUACI√ìN 3: Tabla comparativa completa")
    print("üü°"*40)
    
    compare_models_table(results)
    
    # ========== EVALUACI√ìN 4: AN√ÅLISIS DE CONFUSION MATRICES ==========
    print("\n" + "üü£"*40)
    print("EVALUACI√ìN 4: An√°lisis de confusion matrices")
    print("üü£"*40 + "\n")
    
    print("üìä Analizando matrices de confusi√≥n...")
    print("\nInterpretaci√≥n:")
    print("   ‚Ä¢ True Negatives (TN):  NO rese√±a correctamente identificada")
    print("   ‚Ä¢ True Positives (TP):  Rese√±a correctamente identificada")
    print("   ‚Ä¢ False Positives (FP): NO rese√±a clasificada como rese√±a (Error Tipo I)")
    print("   ‚Ä¢ False Negatives (FN): Rese√±a clasificada como NO rese√±a (Error Tipo II)")
    print()
    
    for model_name, metrics in results.items():
        cm = metrics['confusion_matrix']
        tn, fp, fn, tp = cm.ravel()
        
        print(f"\n{model_name}:")
        print(f"   TN: {tn:6,}  |  FP: {fp:6,}")
        print(f"   FN: {fn:6,}  |  TP: {tp:6,}")
        
        # Calcular error rates
        total = tn + fp + fn + tp
        error_rate = (fp + fn) / total
        fp_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
        fn_rate = fn / (fn + tp) if (fn + tp) > 0 else 0
        
        print(f"   Error Rate:       {error_rate:.2%}")
        print(f"   False Pos Rate:   {fp_rate:.2%}")
        print(f"   False Neg Rate:   {fn_rate:.2%}")
    
    # ========== EVALUACI√ìN 5: AN√ÅLISIS ROC-AUC ==========
    print("\n" + "üî¥"*40)
    print("EVALUACI√ìN 5: An√°lisis ROC-AUC")
    print("üî¥"*40 + "\n")
    
    models_with_roc = {k: v for k, v in results.items() if v['roc_auc']}
    
    if models_with_roc:
        print("üìà Modelos con ROC-AUC (predict_proba disponible):\n")
        
        # Ordenar por ROC-AUC
        sorted_roc = sorted(models_with_roc.items(), key=lambda x: x[1]['roc_auc'], reverse=True)
        
        for model_name, metrics in sorted_roc:
            roc_auc = metrics['roc_auc']
            
            # Interpretaci√≥n de ROC-AUC
            if roc_auc >= 0.95:
                interpretation = "Excelente üèÜ"
            elif roc_auc >= 0.90:
                interpretation = "Muy Bueno ‚úÖ"
            elif roc_auc >= 0.80:
                interpretation = "Bueno ‚úì"
            elif roc_auc >= 0.70:
                interpretation = "Aceptable ‚ö†Ô∏è"
            else:
                interpretation = "Necesita mejora ‚ùå"
            
            print(f"   {model_name:25s}: {roc_auc:.4f} - {interpretation}")
        
        print("\nüí° Interpretaci√≥n ROC-AUC:")
        print("   ‚Ä¢ 1.00:      Clasificador perfecto")
        print("   ‚Ä¢ 0.90-0.99: Excelente")
        print("   ‚Ä¢ 0.80-0.89: Muy bueno")
        print("   ‚Ä¢ 0.70-0.79: Bueno")
        print("   ‚Ä¢ 0.60-0.69: Regular")
        print("   ‚Ä¢ 0.50:      Clasificador aleatorio (sin poder predictivo)")
    else:
        print("‚ö†Ô∏è  Ning√∫n modelo tiene predict_proba disponible.")
        print("   ROC-AUC no puede ser calculado.")
    
    # ========== VISUALIZACIONES ==========
    visualize = input("\n\n¬øDeseas generar visualizaciones? (s/n): ").strip().lower()
    
    if visualize in ['s', 'si', 'yes', 'y']:
        print("\n" + "üé®"*40)
        print("VISUALIZACIONES")
        print("üé®"*40 + "\n")
        
        try:
            # Crear directorio para guardar figuras
            os.makedirs("evaluation_results", exist_ok=True)
            
            # 1. Confusion matrices
            print("üìä Generando confusion matrices...")
            for model_name, metrics in results.items():
                filename = f"evaluation_results/confusion_matrix_{model_name.replace(' ', '_').lower()}.png"
                plot_confusion_matrix(metrics['confusion_matrix'], model_name, filename)
            
            # 2. ROC curves
            if models_with_roc:
                print("\nüìà Generando curvas ROC...")
                plot_roc_curves(results, "evaluation_results/roc_curves.png")
            
            print("\n‚úì Visualizaciones guardadas en: evaluation_results/")
            
        except Exception as e:
            print(f"\n‚ö†Ô∏è  Error al generar visualizaciones: {e}")
            print("   Aseg√∫rate de tener matplotlib y seaborn instalados:")
            print("   pip install matplotlib seaborn")
    
    # ========== RESUMEN FINAL ==========
    print("\n" + "="*80)
    print("‚úÖ EVALUACI√ìN COMPLETADA")
    print("="*80)
    
    # Encontrar mejor modelo
    best_model = max(results.items(), key=lambda x: x[1]['f1_score'])
    
    print("\nüèÜ MEJOR MODELO (por F1-Score):")
    print(f"   Modelo:      {best_model[0]}")
    print(f"   F1-Score:    {best_model[1]['f1_score']:.4f}")
    print(f"   Accuracy:    {best_model[1]['accuracy']:.4f}")
    print(f"   Precision:   {best_model[1]['precision']:.4f}")
    print(f"   Recall:      {best_model[1]['recall']:.4f}")
    if best_model[1]['roc_auc']:
        print(f"   ROC-AUC:     {best_model[1]['roc_auc']:.4f}")
    
    print("\nüí° RECOMENDACIONES:")
    
    # An√°lisis de rendimiento
    f1_score = best_model[1]['f1_score']
    if f1_score >= 0.90:
        print("   ‚úÖ Excelente rendimiento! El modelo est√° listo para producci√≥n.")
    elif f1_score >= 0.85:
        print("   ‚úì Muy buen rendimiento. Considerar ajustes finos opcionales.")
    elif f1_score >= 0.80:
        print("   ‚úì Buen rendimiento. Considerar:")
        print("      - Ajustar hiperpar√°metros (grid search)")
        print("      - Aumentar tama√±o del dataset")
    else:
        print("   ‚ö†Ô∏è  Rendimiento por debajo del objetivo. Acciones sugeridas:")
        print("      - Revisar preprocesamiento de texto")
        print("      - Aumentar features TF-IDF (max_features)")
        print("      - Probar n-gramas m√°s grandes (trigrams)")
        print("      - Balancear mejor el dataset")
        print("      - Ajustar hiperpar√°metros")
    
    # An√°lisis de balance precision-recall
    precision = best_model[1]['precision']
    recall = best_model[1]['recall']
    diff = abs(precision - recall)
    
    print(f"\nüìä Balance Precision-Recall:")
    print(f"   Diferencia: {diff:.4f}")
    
    if diff < 0.05:
        print("   ‚úÖ Excelente balance entre precision y recall")
    elif diff < 0.10:
        print("   ‚úì Buen balance")
    else:
        if precision > recall:
            print("   ‚ö†Ô∏è  Precision > Recall:")
            print("      - El modelo es conservador (pocos falsos positivos)")
            print("      - Pero pierde algunas rese√±as reales (falsos negativos)")
        else:
            print("   ‚ö†Ô∏è  Recall > Precision:")
            print("      - El modelo detecta muchas rese√±as (pocos falsos negativos)")
            print("      - Pero tiene falsos positivos (clasifica no-rese√±as como rese√±as)")
    
    print("\n" + "="*80)
    print("üìÅ Archivos generados:")
    print("   ‚Ä¢ M√©tricas detalladas: Impresas en consola")
    if visualize in ['s', 'si', 'yes', 'y']:
        print("   ‚Ä¢ Visualizaciones: evaluation_results/")
    print("\nüöÄ Siguiente paso: Usar el mejor modelo para predicciones")
    print("   Ver: python example_predict.py")
    print("="*80 + "\n")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Proceso interrumpido por el usuario.")
        sys.exit(0)
    except Exception as e:
        print(f"\n‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
