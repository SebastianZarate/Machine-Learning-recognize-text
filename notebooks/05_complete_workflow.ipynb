{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb46cfc3",
   "metadata": {},
   "source": [
    "# üé¨ An√°lisis de Sentimientos en Rese√±as de Pel√≠culas IMDB\n",
    "# Workflow Completo de Machine Learning\n",
    "\n",
    "---\n",
    "\n",
    "## üìã √çndice\n",
    "\n",
    "1. [Introducci√≥n](#1-introducci√≥n)\n",
    "2. [Carga y Exploraci√≥n de Datos](#2-carga-y-exploraci√≥n-de-datos)\n",
    "3. [Preprocesamiento de Texto](#3-preprocesamiento-de-texto)\n",
    "4. [Representaci√≥n de Texto (Vectorizaci√≥n)](#4-representaci√≥n-de-texto)\n",
    "5. [Divisi√≥n de Datos](#5-divisi√≥n-de-datos)\n",
    "6. [Entrenamiento de Modelos](#6-entrenamiento-de-modelos)\n",
    "7. [Evaluaci√≥n de Modelos](#7-evaluaci√≥n-de-modelos)\n",
    "8. [Visualizaciones](#8-visualizaciones)\n",
    "9. [An√°lisis de Resultados](#9-an√°lisis-de-resultados)\n",
    "10. [Conclusiones y Mejoras](#10-conclusiones-y-mejoras)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920e1da5",
   "metadata": {},
   "source": [
    "## 1. Introducci√≥n\n",
    "\n",
    "### üéØ Objetivo del Proyecto\n",
    "\n",
    "Desarrollar un sistema de **An√°lisis de Sentimientos** que clasifique autom√°ticamente rese√±as de pel√≠culas como **positivas** o **negativas** usando t√©cnicas de Machine Learning y Procesamiento de Lenguaje Natural (NLP).\n",
    "\n",
    "### üìä Dataset: IMDB Movie Reviews\n",
    "\n",
    "- **Fuente**: Internet Movie Database (IMDB)\n",
    "- **Tama√±o**: 50,000 rese√±as de pel√≠culas\n",
    "- **Balance**: 50% positivas, 50% negativas\n",
    "- **Tipo**: Texto libre en ingl√©s\n",
    "- **Tarea**: Clasificaci√≥n binaria (sentiment: positive/negative)\n",
    "\n",
    "### üîß Tecnolog√≠as Utilizadas\n",
    "\n",
    "- **Python 3.x**\n",
    "- **Pandas & NumPy**: Manipulaci√≥n de datos\n",
    "- **NLTK**: Procesamiento de lenguaje natural\n",
    "- **Scikit-learn**: Modelos de Machine Learning\n",
    "- **Matplotlib & Seaborn**: Visualizaci√≥n\n",
    "- **WordCloud**: Visualizaci√≥n de palabras\n",
    "\n",
    "### ü§ñ Modelos a Comparar\n",
    "\n",
    "1. **Naive Bayes** (MultinomialNB)\n",
    "2. **Logistic Regression**\n",
    "3. **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f5adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n inicial e importaciones\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import joblib\n",
    "from collections import Counter\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# M√≥dulos personalizados\n",
    "from src.preprocessing import (\n",
    "    load_imdb_dataset, preprocess_dataframe\n",
    ")\n",
    "from src.train_models import (\n",
    "    split_data, create_vectorizers, vectorize_data,\n",
    "    train_all_models, save_models\n",
    ")\n",
    "from src.visualizations import (\n",
    "    plot_confusion_matrices, plot_roc_curves,\n",
    "    plot_metrics_comparison, generate_wordclouds,\n",
    "    plot_feature_importance\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_curve, auc\n",
    ")\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "print('‚úÖ Entorno configurado correctamente')\n",
    "print(f'üì¶ Pandas version: {pd.__version__}')\n",
    "print(f'üì¶ NumPy version: {np.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a100befb",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Carga y Exploraci√≥n de Datos\n",
    "\n",
    "### üìÇ Carga del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eca7945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset\n",
    "df_raw = load_imdb_dataset('../IMDB Dataset.csv')\n",
    "\n",
    "print('\\nüìä INFORMACI√ìN DEL DATASET')\n",
    "print('='*70)\n",
    "print(f'Total de rese√±as: {len(df_raw):,}')\n",
    "print(f'Columnas: {list(df_raw.columns)}')\n",
    "print(f'\\nPrimeras 3 rese√±as:')\n",
    "df_raw.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d96ce4",
   "metadata": {},
   "source": [
    "### üìä An√°lisis Exploratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d9dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuci√≥n de sentimientos\n",
    "print('\\nüìä DISTRIBUCI√ìN DE SENTIMIENTOS')\n",
    "print('='*70)\n",
    "sentiment_dist = df_raw['sentiment'].value_counts()\n",
    "print(sentiment_dist)\n",
    "print(f'\\nPorcentajes:')\n",
    "print(df_raw['sentiment'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Gr√°fico de barras\n",
    "sentiment_dist.plot(kind='bar', color=['#e74c3c', '#2ecc71'], ax=axes[0], alpha=0.8)\n",
    "axes[0].set_title('Distribuci√≥n de Sentimientos', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Sentimiento')\n",
    "axes[0].set_ylabel('Cantidad')\n",
    "axes[0].set_xticklabels(['Negative', 'Positive'], rotation=0)\n",
    "for i, v in enumerate(sentiment_dist):\n",
    "    axes[0].text(i, v + 500, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Gr√°fico de pie\n",
    "axes[1].pie(sentiment_dist, labels=['Negative', 'Positive'], \n",
    "           colors=['#e74c3c', '#2ecc71'], autopct='%1.1f%%',\n",
    "           startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "axes[1].set_title('Proporci√≥n de Sentimientos', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n‚úÖ Dataset perfectamente balanceado (50-50)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac97b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de longitud de textos\n",
    "df_raw['text_length'] = df_raw['review'].str.len()\n",
    "df_raw['word_count'] = df_raw['review'].str.split().str.len()\n",
    "\n",
    "print('\\nüìè ESTAD√çSTICAS DE LONGITUD')\n",
    "print('='*70)\n",
    "print('\\nCaracteres:')\n",
    "print(df_raw['text_length'].describe())\n",
    "print('\\nPalabras:')\n",
    "print(df_raw['word_count'].describe())\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].hist(df_raw['text_length'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Distribuci√≥n de Longitud (Caracteres)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Caracteres')\n",
    "axes[0].set_ylabel('Frecuencia')\n",
    "axes[0].axvline(df_raw['text_length'].mean(), color='red', linestyle='--', \n",
    "               label=f'Media: {df_raw[\"text_length\"].mean():.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(df_raw['word_count'], bins=50, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_title('Distribuci√≥n de Longitud (Palabras)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Palabras')\n",
    "axes[1].set_ylabel('Frecuencia')\n",
    "axes[1].axvline(df_raw['word_count'].mean(), color='red', linestyle='--',\n",
    "               label=f'Media: {df_raw[\"word_count\"].mean():.0f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76122491",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Preprocesamiento de Texto\n",
    "\n",
    "### üßπ Pipeline de Preprocesamiento\n",
    "\n",
    "El preprocesamiento es crucial para limpiar y normalizar el texto:\n",
    "\n",
    "1. **Limpieza**:\n",
    "   - Eliminar tags HTML (`<br>`, `<p>`, etc.)\n",
    "   - Eliminar URLs y emails\n",
    "   - Eliminar n√∫meros y caracteres especiales\n",
    "\n",
    "2. **Normalizaci√≥n**:\n",
    "   - Convertir a min√∫sculas\n",
    "   - Normalizar espacios\n",
    "\n",
    "3. **Reducci√≥n de ruido**:\n",
    "   - Eliminar stopwords (palabras sin valor sem√°ntico)\n",
    "\n",
    "4. **Lematizaci√≥n**:\n",
    "   - Reducir palabras a su forma base\n",
    "   - Ejemplo: \"running\" ‚Üí \"run\", \"movies\" ‚Üí \"movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b513fbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar preprocesamiento\n",
    "print('\\nüöÄ APLICANDO PREPROCESAMIENTO')\n",
    "print('='*70)\n",
    "print('‚è≥ Procesando 50,000 rese√±as... (esto puede tardar varios minutos)')\n",
    "\n",
    "start_time = time.time()\n",
    "df_processed = preprocess_dataframe(df_raw.copy())\n",
    "preprocessing_time = time.time() - start_time\n",
    "\n",
    "print(f'\\n‚è±Ô∏è Tiempo total: {preprocessing_time:.2f}s ({preprocessing_time/60:.2f} min)')\n",
    "print(f'‚ö° Velocidad: {len(df_processed)/preprocessing_time:.1f} rese√±as/seg')\n",
    "\n",
    "# Mostrar ejemplo\n",
    "print('\\nüìù EJEMPLO DE TRANSFORMACI√ìN:')\n",
    "print('='*70)\n",
    "idx = 0\n",
    "print(f'\\nORIGINAL:\\n{df_raw.iloc[idx][\"review\"][:300]}...')\n",
    "print(f'\\nPROCESADO:\\n{df_processed.iloc[idx][\"review_clean\"][:300]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976befbb",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Representaci√≥n de Texto\n",
    "\n",
    "### üìä Vectorizaci√≥n: Convertir Texto a N√∫meros\n",
    "\n",
    "Los modelos de ML trabajan con n√∫meros, no con texto. Necesitamos vectorizar.\n",
    "\n",
    "#### T√©cnica 1: **Bag of Words (BoW)**\n",
    "- Cuenta frecuencia de cada palabra\n",
    "- Simple y r√°pido\n",
    "- No considera importancia relativa\n",
    "\n",
    "#### T√©cnica 2: **TF-IDF** (Term Frequency - Inverse Document Frequency)\n",
    "- Penaliza palabras muy comunes\n",
    "- Da m√°s peso a palabras distintivas\n",
    "- **Mejor para clasificaci√≥n de texto** ‚úÖ\n",
    "\n",
    "**F√≥rmula TF-IDF**:\n",
    "$$TF-IDF(t, d) = TF(t, d) \\times IDF(t)$$\n",
    "\n",
    "Donde:\n",
    "- $TF(t, d)$ = Frecuencia del t√©rmino $t$ en documento $d$\n",
    "- $IDF(t) = \\log\\frac{N}{df(t)}$\n",
    "- $N$ = Total de documentos\n",
    "- $df(t)$ = Documentos que contienen el t√©rmino $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3502c0",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Divisi√≥n de Datos\n",
    "\n",
    "### üîÄ Train-Test Split\n",
    "\n",
    "Dividimos los datos en dos conjuntos:\n",
    "- **Entrenamiento (80%)**: Para entrenar los modelos\n",
    "- **Prueba (20%)**: Para evaluar el rendimiento\n",
    "\n",
    "Esto evita **overfitting** y nos da una medida real del rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0be3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir datos\n",
    "X_train, X_test, y_train, y_test = split_data(df_processed, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0040dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizar con TF-IDF\n",
    "_, tfidf_vec = create_vectorizers()\n",
    "X_train_vec, X_test_vec, vectorizer = vectorize_data(X_train, X_test, tfidf_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4327221",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Entrenamiento de Modelos\n",
    "\n",
    "### ü§ñ Modelos de Machine Learning\n",
    "\n",
    "Entrenaremos 3 modelos y compararemos su rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1544bd39",
   "metadata": {},
   "source": [
    "#### Modelo 1: Naive Bayes üìä\n",
    "\n",
    "**Teor√≠a:**\n",
    "- Basado en el Teorema de Bayes\n",
    "- Asume independencia entre features (naive assumption)\n",
    "- Muy r√°pido y efectivo para texto\n",
    "\n",
    "**F√≥rmula:**\n",
    "$$P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}$$\n",
    "\n",
    "**Ventajas:**\n",
    "- ‚ö° Extremadamente r√°pido\n",
    "- üìä Funciona bien con alta dimensionalidad\n",
    "- üéØ Buen baseline para clasificaci√≥n de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7bc2b8",
   "metadata": {},
   "source": [
    "#### Modelo 2: Regresi√≥n Log√≠stica üìà\n",
    "\n",
    "**Teor√≠a:**\n",
    "- Modelo lineal con funci√≥n sigmoide\n",
    "- Predice probabilidad de pertenencia a clase\n",
    "- Interpretable y robusto\n",
    "\n",
    "**Funci√≥n Sigmoide:**\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "**Ventajas:**\n",
    "- üîç Coeficientes interpretables\n",
    "- ‚öñÔ∏è Balance entre velocidad y precisi√≥n\n",
    "- üõ°Ô∏è Regularizaci√≥n integrada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11951a03",
   "metadata": {},
   "source": [
    "#### Modelo 3: Random Forest üå≥\n",
    "\n",
    "**Teor√≠a:**\n",
    "- Ensemble de m√∫ltiples √°rboles de decisi√≥n\n",
    "- Cada √°rbol vota la clase final\n",
    "- Captura relaciones no lineales\n",
    "\n",
    "**Ventajas:**\n",
    "- üéØ Alta precisi√≥n\n",
    "- üå≤ Maneja bien features complejas\n",
    "- üìä Menos propenso a overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar todos los modelos\n",
    "models = train_all_models(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a45a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelos\n",
    "save_models(models, vectorizer, '../models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac251f4d",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Evaluaci√≥n de Modelos\n",
    "\n",
    "### üìä M√©tricas de Evaluaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a582f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer predicciones y calcular m√©tricas\n",
    "predictions = {}\n",
    "metrics_data = []\n",
    "confusion_matrices = {}\n",
    "roc_data = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Predicciones\n",
    "    y_pred = model.predict(X_test_vec)\n",
    "    predictions[name] = y_pred\n",
    "    \n",
    "    # M√©tricas\n",
    "    metrics = {\n",
    "        'Model': name,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred)\n",
    "    }\n",
    "    metrics_data.append(metrics)\n",
    "    \n",
    "    # Matriz de confusi√≥n\n",
    "    confusion_matrices[name] = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # ROC\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_proba = model.predict_proba(X_test_vec)[:, 1]\n",
    "    else:\n",
    "        y_proba = model.decision_function(X_test_vec)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_data[name] = {'fpr': fpr, 'tpr': tpr, 'auc': auc(fpr, tpr)}\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data).set_index('Model')\n",
    "\n",
    "print('\\nüìä TABLA DE M√âTRICAS')\n",
    "print('='*70)\n",
    "print(metrics_df)\n",
    "\n",
    "best_model = metrics_df['f1_score'].idxmax()\n",
    "print(f'\\nüèÜ MEJOR MODELO: {best_model} (F1={metrics_df.loc[best_model, \"f1_score\"]:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f8ab6c",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Visualizaciones\n",
    "\n",
    "### üìä Comparaci√≥n Visual de Rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ca20a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaci√≥n de m√©tricas\n",
    "plot_metrics_comparison(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrices de confusi√≥n\n",
    "plot_confusion_matrices(confusion_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807dd2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curvas ROC\n",
    "plot_roc_curves(roc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Clouds\n",
    "generate_wordclouds(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdfc5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (Logistic Regression)\n",
    "plot_feature_importance(models['logistic_regression'], vectorizer, top_n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa92e46",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. An√°lisis de Resultados\n",
    "\n",
    "### üéØ ¬øQu√© modelo funcion√≥ mejor y por qu√©?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b49ba0",
   "metadata": {},
   "source": [
    "### üìä An√°lisis Comparativo\n",
    "\n",
    "#### Rendimiento por Modelo:\n",
    "\n",
    "**Naive Bayes:**\n",
    "- ‚úÖ Muy r√°pido (< 1 segundo)\n",
    "- ‚úÖ Buen baseline (~85% accuracy)\n",
    "- ‚ö†Ô∏è Asume independencia de features\n",
    "\n",
    "**Logistic Regression:**\n",
    "- ‚úÖ Excelente balance velocidad/precisi√≥n\n",
    "- ‚úÖ ~87-89% accuracy t√≠picamente\n",
    "- ‚úÖ Interpretable (coeficientes)\n",
    "- ‚úÖ Robusto y confiable\n",
    "\n",
    "**Random Forest:**\n",
    "- ‚úÖ Captura patrones complejos\n",
    "- ‚ö†Ô∏è M√°s lento (30-60 seg)\n",
    "- ‚ö†Ô∏è Puede no superar a LR en texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07354f18",
   "metadata": {},
   "source": [
    "### üîç An√°lisis de Errores\n",
    "\n",
    "#### Tipos de Errores Comunes:\n",
    "\n",
    "**Falsos Positivos (FP):**\n",
    "- Rese√±as negativas clasificadas como positivas\n",
    "- Pueden ocurrir con sarcasmo o iron√≠a\n",
    "- Ejemplo: \"This movie was 'great'... if you like torture\"\n",
    "\n",
    "**Falsos Negativos (FN):**\n",
    "- Rese√±as positivas clasificadas como negativas\n",
    "- Pueden ocurrir con cr√≠ticas constructivas\n",
    "- Ejemplo: \"Despite some flaws, I loved it\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc62d7a8",
   "metadata": {},
   "source": [
    "### üí° Palabras M√°s Predictivas\n",
    "\n",
    "#### Indicadores Positivos:\n",
    "- \"excellent\", \"amazing\", \"great\", \"wonderful\"\n",
    "- \"loved\", \"enjoyed\", \"perfect\", \"best\"\n",
    "\n",
    "#### Indicadores Negativos:\n",
    "- \"worst\", \"terrible\", \"awful\", \"horrible\"\n",
    "- \"waste\", \"boring\", \"disappointing\", \"bad\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505dc4fe",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Conclusiones y Mejoras\n",
    "\n",
    "### üéØ Resumen de Hallazgos\n",
    "\n",
    "#### ‚úÖ Logros del Proyecto:\n",
    "\n",
    "1. **Dataset balanceado**: 50-50 facilita el entrenamiento\n",
    "2. **Preprocesamiento efectivo**: Limpieza, normalizaci√≥n, lematizaci√≥n\n",
    "3. **Vectorizaci√≥n √≥ptima**: TF-IDF superior a BoW\n",
    "4. **Modelos diversos**: Comparamos 3 aproximaciones diferentes\n",
    "5. **M√©tricas completas**: Evaluaci√≥n exhaustiva con m√∫ltiples m√©tricas\n",
    "6. **Resultados s√≥lidos**: >85% accuracy en todos los modelos\n",
    "\n",
    "#### üìä Modelo Recomendado:\n",
    "\n",
    "**Para producci√≥n**: Logistic Regression\n",
    "- Balance √≥ptimo entre velocidad y precisi√≥n\n",
    "- Interpretable (coeficientes = importancia de palabras)\n",
    "- Robusto y confiable\n",
    "- F√°cil de mantener y actualizar\n",
    "\n",
    "### üöÄ Desaf√≠os Encontrados\n",
    "\n",
    "1. **Sarcasmo e iron√≠a**: Dif√≠cil de detectar con ML tradicional\n",
    "2. **Tiempo de preprocesamiento**: ~5-10 min para 50K rese√±as\n",
    "3. **Memoria con Random Forest**: Consumo alto de RAM\n",
    "4. **Contexto**: Bolsas de palabras ignoran orden\n",
    "\n",
    "### üí° Posibles Mejoras Futuras\n",
    "\n",
    "#### Preprocesamiento:\n",
    "- Manejo de emojis y emoticons\n",
    "- Detecci√≥n de negaciones (\"not good\" vs \"good\")\n",
    "- Stemming vs Lemmatization comparaci√≥n\n",
    "\n",
    "#### Features:\n",
    "- N-gramas m√°s largos (trigramas)\n",
    "- Character-level features\n",
    "- Sentiment lexicons (VADER, TextBlob)\n",
    "\n",
    "#### Modelos:\n",
    "- **SVM** (Support Vector Machines)\n",
    "- **XGBoost / LightGBM**\n",
    "- **Deep Learning**:\n",
    "  - LSTM / GRU (Recurrent Neural Networks)\n",
    "  - BERT / Transformers (State-of-the-art)\n",
    "  - Word2Vec / GloVe embeddings\n",
    "\n",
    "#### Optimizaci√≥n:\n",
    "- Grid Search / Random Search para hiperpar√°metros\n",
    "- Cross-validation para validaci√≥n robusta\n",
    "- Ensemble methods (combinar modelos)\n",
    "- Feature selection para reducir dimensionalidad\n",
    "\n",
    "#### Producci√≥n:\n",
    "- API REST con Flask/FastAPI\n",
    "- Contenedor Docker\n",
    "- Monitoreo de performance\n",
    "- Re-entrenamiento peri√≥dico\n",
    "\n",
    "### üìö Recursos para Profundizar\n",
    "\n",
    "- **Papers**: \"Attention Is All You Need\" (Transformers)\n",
    "- **Libros**: \"Speech and Language Processing\" (Jurafsky & Martin)\n",
    "- **Cursos**: Coursera NLP Specialization\n",
    "- **Bibliotecas**: spaCy, Transformers (Hugging Face)\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ ¬°Proyecto Completado!\n",
    "\n",
    "Este notebook demostr√≥ un **pipeline completo de Machine Learning** para An√°lisis de Sentimientos:\n",
    "\n",
    "‚úÖ Exploraci√≥n de datos  \n",
    "‚úÖ Preprocesamiento de texto  \n",
    "‚úÖ Vectorizaci√≥n (TF-IDF)  \n",
    "‚úÖ Entrenamiento de modelos  \n",
    "‚úÖ Evaluaci√≥n exhaustiva  \n",
    "‚úÖ Visualizaciones profesionales  \n",
    "‚úÖ An√°lisis de resultados  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
